---
title: "Final Project Women ECommerce Clothing Review"
author: "Shalini Sharma, Manvir Singh, Chintan Shah"
date: "12/09/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

This project should allow you to apply the information you've learned in the course to a new dataset. The structure of the final project will be similar to a white paper. The goal of this project is for you to learn how to identify and address a problem with text data and communicate your findings in a report that others can read. The dataset must be related to language or language processing in some way. You must use an analysis we learned in class. 

## Instructions

The final document should be a knitted HTML/PDF/Word document from a Markdown file. You will turn in the knitted document. Be sure to spell and grammar check your work! The following sections should be included: 

### Introduction

Introduce your research topic (the problem) and briefly review related work. Be sure to answer these two questions: (1) Why does this topic matter and (2) What is the background knowledge that someone would need to understand the field or area that you have decided to investigate? In this section, you should include sources that help explain the background area and cite them in APA style. 5-10 articles across the paper would be appropriate - be sure to include these! They are part of the grade! This section should be 3-5 paragraphs.

A) 
#Customer satisfaction in Online Shopping
The first version of electronic shopping was invented in 1979 by connecting a television to a computer via telephone line. Michael Aldrich laid the groundwork for what we now call online shopping.
By 1982, the world's first eCommerce company had been established. The Boston Computer Exchange (BCE) was a website where people could buy and sell used computers. By the mid-1990s, the Internet had established itself as a global communication and connection hub. Netscape, the most popular web browser at the time, had around 10 million users worldwide in 1995. Google released Google AdWords in 2000 as a tool for online advertising that companies could use to market their goods. That is how online marketing or PPC started.

By the 2010s, eCommerce had accelerated significantly. For the first time ever in the history of online shopping, the United States' Cyber Monday sales in 2010 exceeded $1 billion. A recent survey found that 51% of all purchases were made online in 2016, up from 48% in 2015 and roughly 47% in 2014. Another study found that 88% of customers expect businesses to step up their digital efforts. Online testimonials give prospective customers social proof and increase their trust. Additionally, it increases audience trust in the brand. Potential customers are more likely to trust the brand if they are aware of other people's positive experiences with it. Reviews strengthen brand credibility and raise your chances of having customers buy from the organization.

Online reviews can clearly affect the bottom line, according to a Harvard Business School study. Positively perceived brands generate more sales, but even raising your star rating can increase your revenue. The study found that a one-star increase on Yelp.com results in a short-term increase in sales of 5% to 9%. Customer reviews have a huge impact on online shopping, both positive and negative. Positive reviews can encourage potential customers to make a purchase from an online store, while negative reviews can discourage them from doing so. Positive reviews can be especially helpful for online stores, because they act as social proof, showing potential customers that other customers have had a good experience with the store and its products. Negative reviews can be damaging for online stores, as they can result in lost customers and sales. When customers are deciding whether or not to make a purchase from an online store, they often take the time to read customer reviews. This can give them a better understanding of the product, the customer service they can expect to receive, and the overall experience of shopping with the store. Customer reviews can also help online stores to identify areas in which they can improve. If customers are leaving negative reviews, it can provide the store with the opportunity to address any issues that the customers are having. This can help to ensure that customers have a better experience in the future, resulting in more positive reviews. Overall, customer reviews can have a huge impact on online shopping. Through sentiment analysis, online stores can gain insights into customer sentiment, allowing them to identify areas that need improvement and also to leverage positive reviews to encourage more customers to make purchases.

Sentiment analysis is a type of natural language processing that looks at how people feel about a certain topic or product. It can be used to gauge public opinion, to analyze customer feedback, or to monitor social media conversations. It works by analyzing the text of a sentence or paragraph and assigning it a sentiment score—positive, neutral, or negative—based on the words used (Medhat et al., 2014). 
The sentiment score can then be used to gauge a text's overall tone or to assess how well it contrasts with other texts. Customer feedback can be used to generate more insightful insights through sentiment analysis. We frequently want to classify text by topic, which may entail navigating extensive taxonomies of subject matter. On the other hand, sentiment classification typically involves two categories (positive and negative), a range of polarity (for example, star ratings for movies), or even a range in the strength of opinion (Pang & Lee, 2008).

To process and classify datasets containing a variety of reviews for their study, researchers would combine ML and DL models with Natural Language Processing (NLP) methods. NLP's goal is to analyze, extract, and present data so that businesses can make better decisions. In the process of analyzing contentious texts, the level of granularity ranges from individual characters to sub-word units or words forming. This can help them to develop strategies to address customer concerns and to make improvements to their products. Furthermore, sentiment analysis can be used to gauge customer satisfaction with a specific product or service. This can help companies to identify areas where they need to focus on improving their products (Conneau A et al., 2017).

Sentiment analysis has been shown to be effective in a number of applications, including customer reviews. In a study by (Park & Lee, 2015)  authors used a lexical approach to sentiment analysis on customer reviews of hotels, and found that the results were highly accurate. Studies have shown that sentiment analysis can be effective in analyzing customer reviews on online shopping platforms. For example, (Chen et al., 2019) used a combination of machine learning algorithms and rule-based techniques to identify and classify the sentiment of reviews of electronic products on Amazon.com, and found that the approach was effective in capturing both positive and negative sentiment. (Lee et al., 2018) explored the use of sentiment analysis on customer reviews of hotels on TripAdvisor, and found that a hybrid approach combining lexicon-based and machine learning methods was able to accurately classify the sentiment of the reviews. (Kaur & Kaur, 2018) used a machine learning approach to classify the sentiment of customer reviews of mobile phones on Flipkart, and found that the approach was able to accurately capture the sentiment of the reviews.

### Research Question

Define what the problem is and what the goal of the analysis is. Then, specify at least one research question that you answer with your analysis. This section should be a full paragraph.

A) The company here is trying to understand how their customers respond to their products. The goal here is to analyze how strongly the customers feel about thier products. Our research aims at identifying whether customers reviews translates to customer's recommendations for their products. Our analysis can help companies understand the sentiment of their target customers and factors influencing customer satisfaction. Can help strategy teams build understand how the customers describe their products and what type of actions the customers partake in. For analyzing the correlation between the reviews and whether they translate to recommendations, we will perform a Classification Analysis and check whether the strength of the review text matchs the recommendations. To identify which words are associated with Positive, Neutral,and Negative sentiments, we will perform a sentiment analysis.

### Method 

Explain the data you have selected to study. You can find data through many available corpora or other datasets online (ask for help here for sure!). How was the data collected? Who/what is in the data? Identify what the variables are for the analysis. How do these variables fit into your research question?

Describe how you are analyzing the data and why. Here you basically want to justify how your analytic plan will answer your research question. Length of this section will vary, but you should describe your data and analytic plan well enough that someone without a strong background in analytics could understand it.

A) 

```{r libaries}
##r chunk
library(reticulate)
```

```{python}
##python chunk
import re
import numpy as np
#import matplotlib.pyplot as plt
import pandas as pd
from bs4 import BeautifulSoup 
from nltk.stem import PorterStemmer
ps = PorterStemmer()
import nltk
stopwords = nltk.corpus.stopwords.words('english')
import unicodedata
from contractions import contractions_dict
from sklearn.model_selection import train_test_split
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
import gensim
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import spacy
import itertools
import textblob
import string
```

```{python}
##python chunk
df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv')
df.head(5)
```

```{python}
##python chunk
pd.set_option('display.max_columns', None)
```

```{python}
##python chunk
#dropping the row number index
df.drop(columns='RN',inplace=True)
df.head(5)
```

```{python}
##python chunk
df.describe().T
```

```{python}
##python chunk
#Calculating the missing values
df_null = df.isnull().sum()
df_null
#np.round((df.isnull().sum()/len(df))*100,3)
#We found that the title column had the most number of missing values. Since we want to merge the title and the review text we will drop only the rows where both Title and Review Text are missing. 
```

```{python}
##python chunk
#Calculating instances where either only one of Title or Review Text is missing or when both of them are missing.

df_title_missing = df[np.logical_and((df['Title'].isnull()),(df['Review_Text'].notnull()))]
df_title_missing_len = len(df_title_missing)
df_title_missing_len
```

```{python}
##python chunk
df_review_missing = df[np.logical_and((df['Title'].notnull()),(df['Review_Text'].isnull()))]
df_review_missing_len = len(df_review_missing)
df_review_missing_len
```

```{python}
##python chunk
df_both_missing = df[np.logical_and((df['Title'].isnull()),(df['Review_Text'].isnull()))]
df_both_missing_len = len(df_both_missing)
df_both_missing_len
```

```{python}
##python chunk
index = df_both_missing.index
index
```

```{python}
##python chunk
df = df.drop(index)
#df
```

```{python}
##python chunk
index = df_review_missing.index
index
```

```{python}
##python chunk
df = df.drop(index)
#df
```

```{python}
##python chunk
#As we can see from above that Title column has the highest number of missing data(3810) and there is only 1 instance where 
#there a review text is null and title is not null. Hence it is okay to join the Title and Review Text columns.

df['Title'] = df.Title.fillna('')
#df
```

```{python}
##python chunk
df['Title+Review'] = df.Title.str.cat(df.Review_Text,sep = '. ')
df.head(5)
```

```{python}
##python chunk
df.drop(['Title','Review_Text'],axis = 1, inplace=True)
#df.head(5)
```

```{python}
##python chunk
#We are basing the sentiment of the customer's reviews based on the rating they have given.
df['Sentiment_Labels'] = ''
df.loc[df['Rating']>3,'Sentiment_Labels']='Positive'
df.loc[df['Rating']==3,'Sentiment_Labels']='Neutral'
df.loc[df['Rating']<3,'Sentiment_Labels']='Negative'
#df.head(5)
```

```{python}
##python chunk
#Resetting Index
df.reset_index(drop=True,inplace=True)
df.tail(5)
```

```{python}
##python chunk
#Text Cleaning. We are going to clean up the Title+Review column.

stopwords = set(nltk.corpus.stopwords.words('english')) #stopwords
stopwords.remove('no')
stopwords.remove('but')
stopwords.remove('not')

def clean(text):
  text = re.sub("\\t","",text)
  text = re.sub("\\n","",text)
  text = re.sub("\\r","",text)
  text = re.sub("!","",text)
  text = re.sub("\\.+"," ",text)
  text = re.sub("\\s+"," ",text)
  text = re.sub("^\\s+","",text)
  return text

df['Title+Review'] = df['Title+Review'].apply(clean)
#df['Title+Review']

df['Title+Review'] = df['Title+Review'].str.replace('[^a-zA-Z0-9\s]|\[|\]', '')
#df['Title+Review']

#lower case
df['Title+Review'] = df['Title+Review'].str.lower()
#df['Title+Review']

#contractions
for contraction, expansion in contractions_dict.items():
  df['Title+Review'] = df['Title+Review'].str.replace(contraction, expansion)
#df['Title+Review']

#unicode
df['Title+Review'] = [unicodedata.normalize('NFKD', str(text)).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in df['Title+Review'].tolist()]
#df['Title+Review']

#stop words
df['Title+Review'] = [' '.join([word for word in text.split() if word not in stopwords]) for text in df['Title+Review'].tolist()]
df['Title+Review']
```

### Analysis

Analyze the data given your statistical plan. Report the appropriate statistics for that analysis (see lecture notes). Include figures! Include the R-chunks so we can see the analyses you ran and output from the study. Note what you are doing in each step. Length of this section will vary, but you should describe your results well enough that someone without a strong background in analytics could understand it.

A)

```{python}
##python chunk
#We are looking at whether the review text is a good predictor of recommended IND. i.e. based on the reviews text, does the text accurately reflect the person's recommendation of the Recommend_IND variable.For that we need to build a Classification Model.

# Splitting our data into Train and Test data sets. Our predictor variable is 'Title+Review' and dependent variable is Recommend_IND

train_corpus, test_corpus, train_label_names, test_label_names = train_test_split(np.array(df['Title+Review'].apply(lambda x:np.str_(x))),
np.array(df['Recommended_IND']), 
test_size=0.20, 
random_state=42)

train_corpus.shape, test_corpus.shape
trd = dict(Counter(train_label_names))
tsd = dict(Counter(test_label_names))

(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd], 
             columns=['Target Label', 'Train Count', 'Test Count']).sort_values(by=['Train Count', 'Test Count'], ascending=False))
             
```

```{python}
#For feature engineering we'll be using Word2Vec and TF-IDF. We'll be using both models to create inputs for our Classification models.

#Word2Vec Model
#createing the tokenized vectors of the text

tokenized_train = [nltk.tokenize.word_tokenize(text)
                   for text in train_corpus]
tokenized_test = [nltk.tokenize.word_tokenize(text)
                   for text in test_corpus]
```



```{python}
##python chunk
#Building the word2vec model.
import gensim
# build word2vec model
w2v_num_features = 300
w2v_model = gensim.models.Word2Vec(tokenized_train, #corpus
            vector_size=w2v_num_features, #number of features
            window=10, #size of moving window
            min_count=3, #minimum number of times to run
            compute_loss = True,
            sg = 1, #skip-gram model
            epochs=100, workers=5) #iterations and cores
            
#w2v_training_loss = w2v_model.get_latest_training_loss()
#print(w2v_training_loss)            
            
```

```{python}
##python chunk
#Converting the word2vec model into a set of features to use in our classifier. 
def document_vectorizer(corpus, model, num_features):
  vocabulary = set(model.wv.index_to_key)
  def average_word_vectors(words, model, vocabulary, num_features):
    feature_vector = np.zeros((num_features,), dtype="float64")
    nwords = 0.
    for word in words:
      if word in vocabulary: 
        nwords = nwords + 1.
        feature_vector = np.add(feature_vector, model.wv[word])
      if nwords:
        feature_vector = np.divide(feature_vector, nwords)
    return feature_vector
  features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)
                    for tokenized_sentence in corpus]
  return np.array(features)

avg_wv_train_features_w2v = document_vectorizer(corpus=tokenized_train, model=w2v_model,
                                                     num_features=w2v_num_features)
avg_wv_test_features_w2v = document_vectorizer(corpus=tokenized_test, model=w2v_model,
                                                    num_features=w2v_num_features)
                                                    
```

```{python}
##python chunk
#We are using SVM Classifier model for Classification
#SVM Classification Using Word2Vec
svm = LinearSVC(penalty='l2', C=1, random_state=42)
svm.fit(avg_wv_train_features_w2v, train_label_names)
svm_w2v_y_pred = svm.predict(avg_wv_test_features_w2v)
confusion_matrix(test_label_names, svm_w2v_y_pred)
```

```{python}
##python chunk
#print out results of W2V - SVM Classification
print("W2V report for SVM Classification is:") 
print(classification_report(test_label_names, svm_w2v_y_pred))
```

```{python}
##python chunk
#We will also build a Classifier Model using Multinomial Naive Bayes Model
# Since Bayes model doesn't allowed for negative inputs (features) we check for the minimum values of our input features and transform them into positive values 

#avg_wv_train_features_w2v.min()
#avg_wv_test_features_w2v.min()

#Adding 3 to features as MNB cannot take negative values
avg_wv_train_features_w2v = avg_wv_train_features_w2v + 3
avg_wv_test_features_w2v = avg_wv_test_features_w2v + 3

mnb_w2v = MultinomialNB(alpha=1)
mnb_w2v.fit(avg_wv_train_features_w2v, train_label_names)
mnb_w2v_y_pred = svm.predict(avg_wv_test_features_w2v)
confusion_matrix(test_label_names, mnb_w2v_y_pred)

```

```{python}
##python chunk
#print out results of W2V - MNB Classification
print("W2V report for MNB Classification is:") 
print(classification_report(test_label_names, mnb_w2v_y_pred))
```

```{python}
##python chunk
#Now we will perform feature engineering using TF-IDF Model and pass the features as input to our classifier models. For this we first create a TF-IDF Matrix
from sklearn.feature_extraction.text import TfidfVectorizer

# build BOW with TFIDF features on train articles
tv = TfidfVectorizer(use_idf=True, min_df=0.05, max_df=0.95)

# apply to train and test
tv_train_features = tv.fit_transform(train_corpus)
tv_test_features = tv.transform(test_corpus)

# look at feature shape
print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)
```

```{python}
##python chunk
#SVM Using Tf-IDF
svm_tv = LinearSVC(penalty='l2', C=1, random_state=42)
svm_tv.fit(tv_train_features, train_label_names)
svm_tv_y_pred = svm_tv.predict(tv_test_features)
confusion_matrix(test_label_names, svm_tv_y_pred)
```

```{python}
##python chunk
#print out results of TF-IDF - SVM Classification
print("TF-IDF report for SVM Classification is:") 
print(classification_report(test_label_names, svm_tv_y_pred))
```

```{python}
##python chunk
#MNB Using Tf-IDF
mnb_tv =MultinomialNB(alpha=1)
mnb_tv.fit(tv_train_features, train_label_names)
mnb_tv_y_pred = mnb_tv.predict(tv_test_features)
confusion_matrix(test_label_names, mnb_tv_y_pred)
```

```{python}
##python chunk
#print out results of TF-IDF - MNB Classification
print("TF-IDF report for MNB Classification is:") 
print(classification_report(test_label_names, mnb_tv_y_pred))
```

```{python}
##python chunk
#Now we will perform Sentiment Analysis.
#We will use texblob to gauge the Sentiment Polarity of the Reviews. We will create two columns;Sentiment_Polarity and Sentiment_Polarity_1 to gauge the Sentiment Polarity and the labels.

def array_to_list(array):
  my_list = array.tolist()
  sentiment_polarity = []
  for y in my_list:
    sentiment_polarity.append(textblob.TextBlob(y).sentiment.polarity)
  return sentiment_polarity  

df['Sentiment_Polarity'] = array_to_list((df['Title+Review']))

df['Sentiment_Polarity_1'] = ''
df.loc[df['Sentiment_Polarity']>0,'Sentiment_Polarity_1']='Positive'
df.loc[np.logical_and(df['Sentiment_Polarity']<0.2,df['Sentiment_Polarity']>-0.2),'Sentiment_Polarity_1']='Neutral'
df.loc[df['Sentiment_Polarity']<0,'Sentiment_Polarity_1']='Negative'

df.head(5)
```

```{python}
#Creating a classification matrix to compare the results of Rating generated Sentiment Labels and TextBlolb generated Sentiment Labels. Here we are comparing how the two Labels correlate with each other.

confusion_matrix(df['Sentiment_Labels'], df['Sentiment_Polarity_1'])
print(classification_report(y_true=df['Sentiment_Labels'],
                      y_pred=df['Sentiment_Polarity_1'], 
                      labels=['Positive','Neutral','Negative']))
                      
```

```{python}
#Our aim here is to recognize which are most frequently used words grouped by different Sentiment scores. Here we are taking the sentiment labels from the 'Sentiment_Labels' column for analysis. Later we will repeat the exercise for Sentiment_Polarity_1 column and compare the output to check how the Textblob algorithm weighs the adjectives and verbs for different sentiments as compared to user generated sentiments. 
#We first do it for Adjectives
#Here we are looking for most used words in Positive Reviews
nlp = spacy.load('en_core_web_sm')
punctuations = string.punctuation

words_adj = []

for line in df['Title+Review']:
  t = str(line)
  t = t.translate(str.maketrans('', '', string.punctuation))
  doc = nlp(t)
  tokens = list([word.lemma_ for word in doc if word.pos_ == 'ADJ'])
  words_adj.append(tokens)

df['words_adj'] = pd.Series(words_adj)
#df.sample(5)
positive_adj = df[df['Sentiment_Labels']=='Positive']['words_adj']
positive_words = [line for line in positive_adj for line in set(line)]
adj_positive_counts = Counter(positive_words).most_common(20)

print ("The top 20 positive adjectives for human generated Sentiment Labels are:")
print(adj_positive_counts)
```


```{python}
##python chunk
#Here we are looking for most used words in Neutral Reviews
neutral_adj = df[df['Sentiment_Labels']=='Neutral']['words_adj']
neutral_words = [line for line in neutral_adj for line in set(line)]
adj_neutral_counts = Counter(neutral_words).most_common(20)
print ("The top 20 neutral adjectives for human generated Sentiment Labels are:")
print(adj_neutral_counts)
```

```{python}
##python chunk
#Here we are looking for most used words in Negative Reviews
negative_adj = df[df['Sentiment_Labels']=='Negative']['words_adj']
negative_words = [line for line in negative_adj for line in set(line)]
adj_negative_counts = Counter(negative_words).most_common(20)
print ("The top 20 negative adjectives for human generated Sentiment Labels are:")
print(adj_negative_counts)
```

```{python}
##python chunk
# We now do it for Verbs
#Here we are looking for most used words in Positive Reviews
nlp = spacy.load('en_core_web_sm')
punctuations = string.punctuation

words_verb = []

for line in df['Title+Review']:
  t = str(line)
  t = t.translate(str.maketrans('', '', string.punctuation))
  doc = nlp(t)
  tokens = list([word.lemma_ for word in doc if word.pos_ == 'VERB'])
  words_verb.append(tokens)

df['words_verb'] = pd.Series(words_verb)
df.sample(5)
positive_verb = df[df['Sentiment_Labels']=='Positive']['words_verb']
positive_words = [line for line in positive_verb for line in set(line)]
verb_positive_counts = Counter(positive_words).most_common(20)

print ("The top 20 positive verbs for human generated Sentiment Labels are:")
print(verb_positive_counts)
```

```{python}
##python chunk
#Here we look at the top verbs in Neutral Reviews
neutral_verb = df[df['Sentiment_Labels']=='Neutral']['words_verb']
neutral_words = [line for line in neutral_verb for line in set(line)]
verb_neutral_counts = Counter(neutral_words).most_common(20)
print ("The top 20 neutral verbs for human generated Sentiment Labels are:")
print(verb_neutral_counts)
```

```{python}
##python chunk
#Here we look at the top verbs in Negative Reviews
negative_verb = df[df['Sentiment_Labels']=='Negative']['words_verb']
negative_words = [line for line in negative_verb for line in set(line)]
verb_negative_counts = Counter(negative_words).most_common(20)
print ("The top 20 negative verbs for human generated Sentiment Labels are:")
print(verb_negative_counts)
```

```{python}
# We are now doing it for Adjectives for Sentiment_Polarity_1 column (this are the sentiment labels generated from TextBlob algorithm)
#Here we are looking for most used words in Positive Reviews
positive_adj_1 = df[df['Sentiment_Polarity_1']=='Positive']['words_adj']
positive_words_1 = [line for line in positive_adj_1 for line in set(line)]
adj_positive_counts_1 = Counter(positive_words_1).most_common(20)
print ("The top 20 positive adjectives for TextBlob generated Sentiment Labels are:")
print(adj_positive_counts_1)
```

```{python}
##python chunk
#Here we are looking for most used words in Neutral Reviews
neutral_adj_1 = df[df['Sentiment_Polarity_1']=='Neutral']['words_adj']
neutral_words_1 = [line for line in neutral_adj_1 for line in set(line)]
adj_neutral_counts_1 = Counter(neutral_words_1).most_common(20)
print ("The top 20 neutral adjectives for TextBlob generated Sentiment Labels are:")
print(adj_neutral_counts_1)
```

```{python}
##python chunk
#Here we are looking for most used words in Negative Reviews
negative_adj_1 = df[df['Sentiment_Polarity_1']=='Negative']['words_adj']
negative_words_1 = [line for line in negative_adj_1 for line in set(line)]
adj_negative_counts_1 = Counter(negative_words_1).most_common(20)
print ("The top 20 negative adjectives for TextBlob generated Sentiment Labels are:")
print(adj_negative_counts_1)
```

```{python}
##python chunk
# We are now doing it for Verbs for Sentiment_Polarity_1 column (this are the sentiment labels generated from TextBlob algorithm)
#Here we are looking for most used words in Positive Reviews
positive_verb_1 = df[df['Sentiment_Polarity_1']=='Positive']['words_verb']
positive_words_1 = [line for line in positive_verb_1 for line in set(line)]
verb_positive_counts_1 = Counter(positive_words_1).most_common(20)
print ("The top 20 postive verbs for TextBlob generated Sentiment Labels are:")
print(verb_positive_counts_1)
```

```{python}
##python chunk
#Here we look at the top verbs in Neutral Reviews
neutral_verb_1 = df[df['Sentiment_Polarity_1']=='Neutral']['words_verb']
neutral_words_1 = [line for line in neutral_verb_1 for line in set(line)]
verb_neutral_counts_1 = Counter(neutral_words_1).most_common(20)
print ("The top 20 neutral verbs for TextBlob generated Sentiment Labels are:")
print(verb_neutral_counts_1)
```

```{python}
##python chunk
#Here we look at the top verbs in Negative Reviews
negative_verb_1 = df[df['Sentiment_Polarity_1']=='Negative']['words_verb']
negative_words_1 = [line for line in negative_verb_1 for line in set(line)]
verb_negative_counts_1 = Counter(negative_words_1).most_common(20)
print ("The top 20 negative verbs for TextBlob generated Sentiment Labels are:")
print(verb_negative_counts_1)
```

### Discussion

Summarize the results from your study in as plain of language as possible. How does this relate to previous literature? What have we learned from you doing this analysis/study?

Describe the implications of your study and what the practical applications of the findings might be. Here you want to consider what actions might your readers (or company) take based on your conclusions.

A) We performed feature engineering with Word2Vec model and TF-TDF and then used SVM and Multinomial Naive Bayes for classification. Out of the 4 models, we find that the TF-IDF with SVM model gives the most accurate results. It predicts (with 86% accuracy) about how the reviews affect the positive Recommend IND. We see that for all the models our F1-Score for class '0' for all the models is very low. This is because we have a skewed data set where most of the Recommendations are 1. Out TF-IDF Model does a good  job on predicting True Values of 1 as well as detecting false instances of review text indicating a Recommendation indicator having value 1. This analysis would help the company to identify which type of reviews and reviewers are more likely to recommend their products. Next we perform Sentiment Analysis. First step we do is compare the human generated Sentiment Labels (based on classifying the ratings into <3,=3,>3 parameters for Negative, Neutral, and Positive Labels respectively). Then we predict the Sentiment Polarity and similarly divide the polarity between Postiive, Neutral, and Negative Labels. Then we compare them using Classification Matrix, and find out how many reviews that we classified as positive,neutral, and negative are in conjunction with the model generated labels. We find that our overall match is around 66% with highest match for Positive results. The relatively lower match for Neutral and Negative Reviews may possibly due to the bias of more Positive reviews in the data set. After the comparison we then used the POS Tagging along with Sentiment Analysis to identify which are the most common adjectives and verbs used by reviewers for their Positive, Neutral, and Negative Reviews. For both, Adjective and Verbs, we found that both Neutral and Negative reviews had more words in common with the Positive reviews for the human generated Sentiment Labels (based on the direct ratings classification). This tells us that human classification on unknown data would be a misleading indicator as there are many times where people's reviews don't truly reflect their ratings and people's sentiment is different from their actions. The Textblob generated adjectives and verbs give a higher count of negative and neutral words for the reviews which indicates that the model does a better job of aligning the reviews with the ratings, and hence would be the correct way forward when it is presented with real-world untrained data.     

### Credit Guidelines

If working in a group, specify what each member of the group did. For each of the steps below, specify who did the work. If more than one member worked on the step, put names of everyone who worked on it.

- Developed Research Question - Shalini Sharma, Manvir Singh, Chintan Shah
- Acquired Data - Shalini Sharma, Manvir Singh, Chintan Shah
- Analyzed Data - Chintan Shah
- Researched Related Work - Chintan Shah
- Wrote Introduction - Manvir Singh
- Wrote Method - Shalini Sharma
- Wrote Analysis/Results - Chintan Shah
- Wrote Discussion - Shalini Sharma, Manvir Singh, Chintan Shah
- Revisions/Edits - Shalini Sharma, Manvir Singh, Chintan Shah

### References

Include your references in APA style.

1)Chen, J., Dai, W., Zhang, Y., & Chen, Z. (2019). Sentiment analysis on Amazon customer reviews of electronic products. Expert Systems with Applications, 113–121.

2)Chevalier, J. A., & Mayzlin, D. (2006). The effect of word of mouth on sales: Online book reviews. Journal of Marketing Research, 3, 345–354.

3)Conneau A, Schwenk H, Barrault L, & Lecun Y. (2017). Very deep convolutional networks for text classification.

4)Kaur, G., & Kaur, D. (2018). Sentiment analysis of customer reviews on Flipkart for mobile phones. International Journal of Engineering and Technology, 7, 489–492.

5)Lee, Juhyun and Lee, Kyungmi and Lee, & Jaehoon. (2018). Sentiment analysis on customer reviews of hotels on TripAdvisor. International Journal of Hospitality Management, 72, 94–103.

6)Medhat, W., Hassan, A., & Korashy, H. (2014). Sentiment analysis algorithms and applications: A survey. Ain Shams Engineering Journal, 5(4), 1093–1113. https://doi.org/https://doi.org/10.1016/j.asej.2014.04.011

7)Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. . Foundation and Trends in Information Retrieval, 1–135.

8)Park, S., & Lee, K. (2015). Sentiment analysis of hotel reviews using a lexical approach. . International Journal of Hospitality Management, 88–97.